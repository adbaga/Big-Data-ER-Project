{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86ccda3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MONSTER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "path = os.getcwd()\n",
    "from classes.Block import Block #the block class\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75d93b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO (put done if it is done)\n",
    "\n",
    "#variables to create:\n",
    "#all tokens of e1: val_e1_tokens ([str]) DONE\n",
    "#all tokens of e1: val_e2_tokens [str] DONE\n",
    "#Tokens in each entityProfile of E1 ({ \"eP\": ['token1', 'token2',...'tokenN']}): dict_E1 DONE\n",
    "#Tokens in each entityProfile of E2 ({ \"eP\": ['token1', 'token2',...'tokenN']}): dict_E2 DONE\n",
    "#sets of token set(tokenE1 + tokenE2): commonToken DONE\n",
    "#Entities of E1 that contains a ct ∈ commonToken { \"cT\": [1,2,3,10,....]}: e1_dict_token DONE\n",
    "#Entities of E2 that contains a ct ∈ commonToken { \"cT\": [101,102,103,110,....]}: e2_dict_token DONE\n",
    "#Attributes of E1: all_att_e1 DONE\n",
    "#Attributes of E2: all_att_e2 DONE\n",
    "\n",
    "\n",
    "#make a class for the block: DONE\n",
    "#create the class in a seperate python file/module (?) DONE\n",
    "#Block needs a: name, ib1 (innerBlock1), ib2 (innerBlock2). if one of the ib ∈ IB is empty, a block\n",
    "#of a token should not be created\n",
    "#in addition, cardinality and block size\n",
    "\n",
    "\n",
    "#stemming and cleaning:\n",
    "#Do it at stem clean function\n",
    "# remove stop words DONE\n",
    "# stemming\n",
    "# tokenize DONE\n",
    "# DISCUSS: ignore numbers, so take only words as token, not integer\n",
    "#AND LEMMATIZATION\n",
    "#combine lowering all letters before making the token in one stem_clean function \n",
    "#so it will be easier to implement any semantic treatment\n",
    "#delete period signs\n",
    "\n",
    "#visualizatipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f495c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f1 = open(f'{path}/data/entityCol1.json')\n",
    "f2 = open(f'{path}/data/entityCol2.json')\n",
    " \n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "ec1 = json.load(f1)\n",
    "ec2 = json.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2243952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_clean(arr):\n",
    "    joint_words = ' '.join(arr)\n",
    "    joint_words = str(joint_words).lower()\n",
    "    val = word_tokenize(joint_words)\n",
    "    valTok = list(set([w for w in val if not w.lower() in stop_words]))\n",
    "    return valTok\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c79e499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all unique key and val on both EC\n",
    "all_att_e1 = []\n",
    "all_val_e1 = []\n",
    "dict_E1 = {}\n",
    "x = 1\n",
    "for d in ec1:\n",
    "    values_EP = []\n",
    "    for key, val in (d.items()):\n",
    "        all_att_e1.append(key) #allKey\n",
    "        treatedVal = str(val).lower()\n",
    "        values_EP.append(treatedVal)\n",
    "        all_val_e1.append(treatedVal) #allVal - treat number as string\n",
    "    values_EP = stem_clean(values_EP)\n",
    "    dict_E1[x] = values_EP\n",
    "    x = x+1\n",
    "\n",
    "attE1Keys = list(set(all_att_e1))\n",
    "val_e1_tokens = list(set(stem_clean(all_val_e1)))\n",
    "\n",
    "\n",
    "all_at_e2 = [] \n",
    "all_val_e2 = []\n",
    "dict_E2 = {}\n",
    "x = 101\n",
    "for d in ec2:\n",
    "    values_EP = []\n",
    "    for key, val in (d.items()):\n",
    "        all_at_e2.append(key) #allKey\n",
    "        treatedVal = str(val).lower()\n",
    "        values_EP.append(treatedVal)\n",
    "        all_val_e2.append(treatedVal) #allVal - treat number as string\n",
    "    values_EP = stem_clean(values_EP)\n",
    "    dict_E2[x] = values_EP\n",
    "    x = x+1\n",
    "attE2Keys = list(set(all_at_e2))\n",
    "val_e2_tokens = list(set(stem_clean(all_val_e2)))\n",
    "\n",
    "\n",
    "#common token\n",
    "#Only take token that exists in both "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761bc7ec",
   "metadata": {},
   "source": [
    "\n",
    "<h3>Token Blocking </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95240077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#common token\n",
    "#Only take token that exists in both \n",
    "common_token = set(val_e1_tokens) & set(val_e2_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60ff59cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the common token create find all entity profile\n",
    "#that has contains the token\n",
    "e1_dict_token = {}\n",
    "for t in common_token:\n",
    "    x = [ ep for ep in dict_E1 if t in dict_E1[ep]]\n",
    "    e1_dict_token[t] = x\n",
    "    \n",
    "e2_dict_token = {}\n",
    "for t in common_token:\n",
    "    x = [ ep for ep in dict_E2 if t in dict_E2[ep]]\n",
    "    e2_dict_token[t] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b1ea068",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_blocks = []\n",
    "for ct in common_token:\n",
    "    new_block = Block(ct,e1_dict_token[ct], e2_dict_token[ct] )\n",
    "    all_blocks.append(new_block)\n",
    "    \n",
    "all_blocks.sort(key=lambda x: x.b_cardinality, reverse=True) #sort based on its cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1ecc949",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Block' object has no attribute 'b_name'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_6452\\2154069574.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m#DataFrame for purpose Visualization Purpose\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mtoken_blocks_df\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mas_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mall_blocks\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mtoken_blocks_df\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m20\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_6452\\2154069574.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m#DataFrame for purpose Visualization Purpose\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mtoken_blocks_df\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mas_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mall_blocks\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mtoken_blocks_df\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m20\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Lectures\\Big Data Entity Resolution\\Project\\Big-Data-ER-Project\\classes\\Block.py\u001B[0m in \u001B[0;36mas_dict\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mas_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 14\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;34m'block_name'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mb_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'int_b1'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mib1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'int_b1_size'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mib1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'int_b2'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mib2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'int_b2_size'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mib2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'b_cardin'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mb_cardinality\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'b_size'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mb_size\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m: 'Block' object has no attribute 'b_name'"
     ]
    }
   ],
   "source": [
    "#DataFrame for purpose Visualization Purpose\n",
    "token_blocks_df = pd.DataFrame([x.as_dict() for x in all_blocks])\n",
    "token_blocks_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2452ca81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9d77836",
   "metadata": {},
   "source": [
    "<h3>Attribute Clustering</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41161a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It's not finished yet, please do not proceed, let me finish it on Monday. :D\n",
    "\"\"\"\n",
    "\n",
    "#Group based on the attribute for E1\n",
    "#Group based on the attribute for E2\n",
    "\"\"\"attE1Keys.remove('id')\n",
    "attE2Keys.remove('id')\n",
    "attE2Keys.remove('age')\"\"\"\n",
    "\n",
    "attribute_blocks_ec1 = {}\n",
    "attribute_blocks_ec2 = {}\n",
    "for attribute in attE1Keys:\n",
    "    attribute_blocks_ec1[attribute] = stem_clean(word_tokenize(' '.join(set([att[attribute].lower() for att in ec1 if attribute in att.keys()]))))\n",
    "for attribute in attE2Keys:\n",
    "    attribute_blocks_ec2[attribute] = stem_clean(word_tokenize(' '.join(set([att[attribute].lower() for att in ec2 if attribute in att.keys()]))))\n",
    "\n",
    "#Similarity Calculation\n",
    "\n",
    "def jaccard_set(list1, list2):\n",
    "    \"\"\"Define Jaccard Similarity function for two sets\"\"\"\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union\n",
    "\n",
    "sim_ec1_ec2 = {}\n",
    "for each in attribute_blocks_ec1:\n",
    "    temp = []\n",
    "    for every in attribute_blocks_ec2:\n",
    "        similarity = jaccard_set(attribute_blocks_ec1[each], attribute_blocks_ec2[every])\n",
    "        if similarity > 0:\n",
    "            temp.append(every)\n",
    "    sim_ec1_ec2[each] = temp\n",
    "\n",
    "sim_ec2_ec1 = {}\n",
    "for each in attribute_blocks_ec2:\n",
    "    temp = []\n",
    "    for every in attribute_blocks_ec1:\n",
    "        similarity = jaccard_set(attribute_blocks_ec2[each], attribute_blocks_ec1[every])\n",
    "        if similarity > 0:\n",
    "            temp.append(every)\n",
    "    sim_ec2_ec1[each] = temp\n",
    "\n",
    "#Attribute Blocking\n",
    "\n",
    "\n",
    "#testa = jaccard_set(attribute_blocks_ec1['domicile'], attribute_blocks_ec2['country'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}