{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86ccda3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kalevkim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "path = os.getcwd()\n",
    "from classes.Block import Block #the block class\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75d93b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO (put done if it is done)\n",
    "\n",
    "#variables to create:\n",
    "#all tokens of e1: val_e1_tokens ([str]) DONE\n",
    "#all tokens of e1: val_e2_tokens [str] DONE\n",
    "#Tokens in each entityProfile of E1 ({ \"eP\": ['token1', 'token2',...'tokenN']}): dict_E1 DONE\n",
    "#Tokens in each entityProfile of E2 ({ \"eP\": ['token1', 'token2',...'tokenN']}): dict_E2 DONE\n",
    "#sets of token set(tokenE1 + tokenE2): commonToken\n",
    "#Entities of E1 that contains a ct ∈ commonToken { \"cT\": [1,2,3,10,....]}: e1DictToken\n",
    "#Entities of E2 that contains a ct ∈ commonToken { \"cT\": [101,102,103,110,....]}: e2DictToken\n",
    "#Attributes of E1: all_att_e1\n",
    "#Attributes of E2: all_att_e2\n",
    "\n",
    "\n",
    "#make a class for the block:\n",
    "#create the class in a seperate python file/module (?)\n",
    "#Block needs a: name, ib1 (innerBlock1), ib2 (innerBlock2). if one of the ib ∈ IB is empty, a block\n",
    "#of a token should not be created\n",
    "\n",
    "\n",
    "#stemming and cleaning:\n",
    "#Do it at stem clean function\n",
    "# remove stop words DONE\n",
    "# stemming\n",
    "# tokenize DONE\n",
    "# DISCUSS: ignore numbers, so take only words as token, not integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f495c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f1 = open(f'{path}/data/entityCol1.json')\n",
    "f2 = open(f'{path}/data/entityCol2.json')\n",
    " \n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "ec1 = json.load(f1)\n",
    "ec2 = json.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2243952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_clean(arr):\n",
    "    joint_words = ' '.join(arr)\n",
    "    val = word_tokenize(joint_words)\n",
    "    valTok = list(set([w for w in val if not w.lower() in stop_words]))\n",
    "    return valTok\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c79e499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all unique key and val on both EC\n",
    "all_att_e1 = []\n",
    "all_val_e1 = []\n",
    "dict_E1 = {}\n",
    "x = 1\n",
    "for d in ec1:\n",
    "    values_EP = []\n",
    "    for key, val in (d.items()):\n",
    "        all_att_e1.append(key) #allKey\n",
    "        treatedVal = str(val).lower()\n",
    "        values_EP.append(treatedVal)\n",
    "        all_val_e1.append(treatedVal) #allVal - treat number as string\n",
    "    values_EP = stem_clean(values_EP)\n",
    "    dict_E1[x] = values_EP\n",
    "    x = x+1\n",
    "\n",
    "attE1Keys = list(set(all_att_e1))\n",
    "val_e1_tokens = list(set(stem_clean(all_val_e1)))\n",
    "\n",
    "\n",
    "all_at_e2 = [] \n",
    "all_val_e2 = []\n",
    "dict_E2 = {}\n",
    "x = 101\n",
    "for d in ec2:\n",
    "    values_EP = []\n",
    "    for key, val in (d.items()):\n",
    "        all_at_e2.append(key) #allKey\n",
    "        treatedVal = str(val).lower()\n",
    "        values_EP.append(treatedVal)\n",
    "        all_val_e2.append(treatedVal) #allVal - treat number as string\n",
    "    values_EP = stem_clean(values_EP)\n",
    "    dict_E2[x] = values_EP\n",
    "    x = x+1\n",
    "attE2Keys = list(set(all_at_e2))\n",
    "val_e2_tokens = list(set(stem_clean(all_val_e2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761bc7ec",
   "metadata": {},
   "source": [
    "\n",
    "<h3>Token Blocking </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95240077",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
