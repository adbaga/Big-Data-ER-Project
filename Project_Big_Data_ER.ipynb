{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "86ccda3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kalevkim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "path = os.getcwd()\n",
    "from classes.Block import Block #the block class\n",
    "from classes.Attribute import Attribute #the block class\n",
    "import networkx as nx\n",
    "import itertools\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f495c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f1 = open(f'{path}/data/entityCol1.json')\n",
    "f2 = open(f'{path}/data/entityCol2.json')\n",
    " \n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "ec1 = json.load(f1)\n",
    "ec2 = json.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2243952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(arr):\n",
    "    arr = [str(i) for i in arr]\n",
    "    joint_words = ' '.join(arr)\n",
    "    joint_words = str(joint_words).lower()\n",
    "    val = word_tokenize(joint_words)\n",
    "    valTok = list(set([w for w in val if not w.lower() in stop_words and len(w) >= 3]))\n",
    "    return valTok\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c08db98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to make attribute extraction, dictionary_token making, and unique token value\n",
    "#still being experimented\n",
    "\n",
    "def extract_unique_key_val(start_ind: int, ec ):\n",
    "    all_att_e = []\n",
    "    all_val_e = []\n",
    "    dict_E = {}\n",
    "    ep_ids = []\n",
    "    x = start_ind\n",
    "    for d in ec:\n",
    "        values_EP = []\n",
    "        for key, val in (d.items()):\n",
    "            if(key != 'id'): #exclude id into the calculation. ID is just for marking\n",
    "                all_att_e.append(key) #allKey\n",
    "                treatedVal = str(val).lower()\n",
    "                values_EP.append(treatedVal)\n",
    "                all_val_e.append(treatedVal) #allVal - treat number as string\n",
    "            else:\n",
    "                \n",
    "                ep_ids.append(str(val).lower())\n",
    "            \n",
    "        values_EP = text_cleaning(values_EP)\n",
    "        dict_E[x] = values_EP\n",
    "        x = x+1\n",
    "    \n",
    "    att_e_keys = list(set(all_att_e))\n",
    "    val_e_tokens = list(set(text_cleaning(all_val_e)))\n",
    "    \n",
    "    \n",
    "    return (att_e_keys, val_e_tokens, dict_E, ep_ids)\n",
    "    \n",
    "    \n",
    "attE1Keys, val_e1_tokens, dict_E1, e1_ep_ids = extract_unique_key_val(1, ec1)\n",
    "attE2Keys, val_e2_tokens, dict_E2, e2_ep_ids = extract_unique_key_val(101, ec2)\n",
    "all_ids = e1_ep_ids + e2_ep_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761bc7ec",
   "metadata": {},
   "source": [
    "\n",
    "<h3>Token Blocking </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f415d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_blocking(val_e1_tokens, val_e2_tokens, dict_E1, dict_E2):\n",
    "    #common token\n",
    "    #Only take token that exists in both \n",
    "    \n",
    "    common_token = set(val_e1_tokens) & set(val_e2_tokens)\n",
    "    \n",
    "    \n",
    "    #from the common token create find all entity profile\n",
    "    #that has contains the token\n",
    "    e1_dict_token = {}\n",
    "    \n",
    "    for t in common_token:\n",
    "        x = [ ep for ep in dict_E1 if t in dict_E1[ep]]\n",
    "        e1_dict_token[t] = x\n",
    "    \n",
    "\n",
    "    e2_dict_token = {}\n",
    "    for t in common_token:\n",
    "        x = [ ep for ep in dict_E2 if t in dict_E2[ep]]\n",
    "        e2_dict_token[t] = x\n",
    "    \n",
    "    all_blocks = []\n",
    "    for ct in common_token:\n",
    "        new_block = Block(ct,e1_dict_token[ct], e2_dict_token[ct] )\n",
    "        all_blocks.append(new_block)\n",
    "\n",
    "    all_blocks.sort(key=lambda x: x.b_cardinality, reverse=True) #sort based on its cardinality\n",
    "    \n",
    "    return all_blocks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "95240077",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_blocks_all = token_blocking(val_e1_tokens, val_e2_tokens, dict_E1, dict_E2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d1ecc949",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>block_name</th>\n",
       "      <th>inner_b1</th>\n",
       "      <th>in_b1_size</th>\n",
       "      <th>inner_b2</th>\n",
       "      <th>in_b2_size</th>\n",
       "      <th>b_cardin</th>\n",
       "      <th>b_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>foundation</td>\n",
       "      <td>[10, 13, 15, 16, 43, 57, 77, 80, 83]</td>\n",
       "      <td>9</td>\n",
       "      <td>[106, 107, 125, 128, 141, 144, 152, 161, 170, ...</td>\n",
       "      <td>17</td>\n",
       "      <td>153</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>limited</td>\n",
       "      <td>[11, 20, 25, 38, 42, 64, 68, 87, 92, 97]</td>\n",
       "      <td>10</td>\n",
       "      <td>[111, 116, 117, 120, 164, 168, 199, 200]</td>\n",
       "      <td>8</td>\n",
       "      <td>80</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>institute</td>\n",
       "      <td>[7, 12, 19, 29, 79, 82, 89]</td>\n",
       "      <td>7</td>\n",
       "      <td>[104, 110, 127, 148, 159, 167, 171, 190, 194, ...</td>\n",
       "      <td>11</td>\n",
       "      <td>77</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>associates</td>\n",
       "      <td>[21, 53, 56, 58, 59, 60, 69, 73]</td>\n",
       "      <td>8</td>\n",
       "      <td>[112, 121, 132, 136, 146, 162, 163, 172]</td>\n",
       "      <td>8</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llc</td>\n",
       "      <td>[18, 47, 62, 72, 76, 81, 94]</td>\n",
       "      <td>7</td>\n",
       "      <td>[109, 124, 126, 134, 150, 158, 169, 173, 174]</td>\n",
       "      <td>9</td>\n",
       "      <td>63</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>corporation</td>\n",
       "      <td>[3, 9, 24, 26, 27, 36, 37, 39, 49, 63]</td>\n",
       "      <td>10</td>\n",
       "      <td>[115, 131, 140, 181, 182, 191]</td>\n",
       "      <td>6</td>\n",
       "      <td>60</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>incorporated</td>\n",
       "      <td>[1, 6, 30, 46, 54, 84]</td>\n",
       "      <td>6</td>\n",
       "      <td>[103, 129, 138, 145, 156, 187, 188, 189]</td>\n",
       "      <td>8</td>\n",
       "      <td>48</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ltd</td>\n",
       "      <td>[8, 35, 71, 74, 75, 88, 93, 96]</td>\n",
       "      <td>8</td>\n",
       "      <td>[123, 149, 154, 155, 165, 180]</td>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>industries</td>\n",
       "      <td>[5, 31, 45, 51, 52, 55, 78]</td>\n",
       "      <td>7</td>\n",
       "      <td>[102, 130, 137, 143, 183, 193]</td>\n",
       "      <td>6</td>\n",
       "      <td>42</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>italy</td>\n",
       "      <td>[14, 16, 22, 41, 45, 77]</td>\n",
       "      <td>6</td>\n",
       "      <td>[105, 107, 113, 135, 137, 141]</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>spain</td>\n",
       "      <td>[6, 9, 25, 49]</td>\n",
       "      <td>4</td>\n",
       "      <td>[103, 116, 133, 136, 145, 172, 173, 174]</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>female</td>\n",
       "      <td>[43, 67, 68, 69, 76, 82]</td>\n",
       "      <td>6</td>\n",
       "      <td>[104, 118, 119, 121, 127]</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mauris</td>\n",
       "      <td>[46, 71, 72]</td>\n",
       "      <td>3</td>\n",
       "      <td>[123, 124, 144, 163, 173, 175, 187]</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pellentesque</td>\n",
       "      <td>[7, 23, 48, 82]</td>\n",
       "      <td>4</td>\n",
       "      <td>[104, 114, 127, 199, 200]</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>libero</td>\n",
       "      <td>[26, 27, 36, 37, 39, 49]</td>\n",
       "      <td>6</td>\n",
       "      <td>[136, 143, 178]</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nunc</td>\n",
       "      <td>[38, 42, 60]</td>\n",
       "      <td>3</td>\n",
       "      <td>[117, 150, 164, 175, 178]</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>non</td>\n",
       "      <td>[7, 14, 44, 57, 79]</td>\n",
       "      <td>5</td>\n",
       "      <td>[104, 105, 149]</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sed</td>\n",
       "      <td>[5, 78]</td>\n",
       "      <td>2</td>\n",
       "      <td>[102, 141, 150, 154, 165, 178, 181]</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>llp</td>\n",
       "      <td>[28, 41, 66, 85]</td>\n",
       "      <td>4</td>\n",
       "      <td>[118, 147, 195]</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>massa</td>\n",
       "      <td>[45, 51, 52, 56]</td>\n",
       "      <td>4</td>\n",
       "      <td>[169, 183, 195]</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      block_name                                  inner_b1  in_b1_size  \\\n",
       "0     foundation      [10, 13, 15, 16, 43, 57, 77, 80, 83]           9   \n",
       "1        limited  [11, 20, 25, 38, 42, 64, 68, 87, 92, 97]          10   \n",
       "2      institute               [7, 12, 19, 29, 79, 82, 89]           7   \n",
       "3     associates          [21, 53, 56, 58, 59, 60, 69, 73]           8   \n",
       "4            llc              [18, 47, 62, 72, 76, 81, 94]           7   \n",
       "5    corporation    [3, 9, 24, 26, 27, 36, 37, 39, 49, 63]          10   \n",
       "6   incorporated                    [1, 6, 30, 46, 54, 84]           6   \n",
       "7            ltd           [8, 35, 71, 74, 75, 88, 93, 96]           8   \n",
       "8     industries               [5, 31, 45, 51, 52, 55, 78]           7   \n",
       "9          italy                  [14, 16, 22, 41, 45, 77]           6   \n",
       "10         spain                            [6, 9, 25, 49]           4   \n",
       "11        female                  [43, 67, 68, 69, 76, 82]           6   \n",
       "12        mauris                              [46, 71, 72]           3   \n",
       "13  pellentesque                           [7, 23, 48, 82]           4   \n",
       "14        libero                  [26, 27, 36, 37, 39, 49]           6   \n",
       "15          nunc                              [38, 42, 60]           3   \n",
       "16           non                       [7, 14, 44, 57, 79]           5   \n",
       "17           sed                                   [5, 78]           2   \n",
       "18           llp                          [28, 41, 66, 85]           4   \n",
       "19         massa                          [45, 51, 52, 56]           4   \n",
       "\n",
       "                                             inner_b2  in_b2_size  b_cardin  \\\n",
       "0   [106, 107, 125, 128, 141, 144, 152, 161, 170, ...          17       153   \n",
       "1            [111, 116, 117, 120, 164, 168, 199, 200]           8        80   \n",
       "2   [104, 110, 127, 148, 159, 167, 171, 190, 194, ...          11        77   \n",
       "3            [112, 121, 132, 136, 146, 162, 163, 172]           8        64   \n",
       "4       [109, 124, 126, 134, 150, 158, 169, 173, 174]           9        63   \n",
       "5                      [115, 131, 140, 181, 182, 191]           6        60   \n",
       "6            [103, 129, 138, 145, 156, 187, 188, 189]           8        48   \n",
       "7                      [123, 149, 154, 155, 165, 180]           6        48   \n",
       "8                      [102, 130, 137, 143, 183, 193]           6        42   \n",
       "9                      [105, 107, 113, 135, 137, 141]           6        36   \n",
       "10           [103, 116, 133, 136, 145, 172, 173, 174]           8        32   \n",
       "11                          [104, 118, 119, 121, 127]           5        30   \n",
       "12                [123, 124, 144, 163, 173, 175, 187]           7        21   \n",
       "13                          [104, 114, 127, 199, 200]           5        20   \n",
       "14                                    [136, 143, 178]           3        18   \n",
       "15                          [117, 150, 164, 175, 178]           5        15   \n",
       "16                                    [104, 105, 149]           3        15   \n",
       "17                [102, 141, 150, 154, 165, 178, 181]           7        14   \n",
       "18                                    [118, 147, 195]           3        12   \n",
       "19                                    [169, 183, 195]           3        12   \n",
       "\n",
       "    b_size  \n",
       "0       26  \n",
       "1       18  \n",
       "2       18  \n",
       "3       16  \n",
       "4       16  \n",
       "5       16  \n",
       "6       14  \n",
       "7       14  \n",
       "8       13  \n",
       "9       12  \n",
       "10      12  \n",
       "11      11  \n",
       "12      10  \n",
       "13       9  \n",
       "14       9  \n",
       "15       8  \n",
       "16       8  \n",
       "17       9  \n",
       "18       7  \n",
       "19       7  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DataFrame for Visualization Purpose\n",
    "#each row represents one block\n",
    "token_blocks_df = pd.DataFrame([x.as_dict() for x in token_blocks_all])\n",
    "token_blocks_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d77836",
   "metadata": {},
   "source": [
    "<h3>Attribute Clustering</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "41161a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group based on the attribute for E1\n",
    "#Group based on the attribute for E2\n",
    "\n",
    "attribute_blocks_ec1 = {}\n",
    "attribute_blocks_ec2 = {}\n",
    "\n",
    "#get all values for the attributes\n",
    "for attribute in attE1Keys:\n",
    "    attribute_blocks_ec1[attribute]= text_cleaning(list(set([att[attribute] for att in ec1 if attribute in att.keys()])))\n",
    "\n",
    "for attribute in attE2Keys:\n",
    "    attribute_blocks_ec2[attribute] = text_cleaning(list(set([att[attribute] for att in ec2 if attribute in att.keys()])))\n",
    "\n",
    "# Similarity Calculation\n",
    "\n",
    "def jaccard_set(list1, list2):\n",
    "    \"\"\"Define Jaccard Similarity function for two sets\"\"\"\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union\n",
    "\n",
    "sim_ec1_ec2 = {}\n",
    "for each in attribute_blocks_ec1:\n",
    "    dict_temp = {}\n",
    "    for every in attribute_blocks_ec2:\n",
    "        similarity = jaccard_set(attribute_blocks_ec1[each], attribute_blocks_ec2[every])\n",
    "        if similarity > 0:\n",
    "            dict_temp[every] = similarity\n",
    "    if(bool(dict_temp)): #check if the dictionary is not empty\n",
    "        max_key = max(dict_temp, key=dict_temp.get)\n",
    "        sim_ec1_ec2[each] = max_key\n",
    "\n",
    "\n",
    "sim_ec2_ec1 = {}\n",
    "for each in attribute_blocks_ec2:\n",
    "    dict_temp = {}\n",
    "    for every in attribute_blocks_ec1:\n",
    "        similarity = jaccard_set(attribute_blocks_ec2[each], attribute_blocks_ec1[every])\n",
    "        if similarity > 0:\n",
    "            dict_temp[every] = similarity\n",
    "    if(bool(dict_temp)): #check if the dictionary is not empty\n",
    "        max_key = max(dict_temp, key=dict_temp.get)\n",
    "        sim_ec2_ec1[each] = max_key\n",
    "        \n",
    "G = nx.Graph()\n",
    "\n",
    "for each in sim_ec1_ec2:\n",
    "    G.add_node(each)\n",
    "    G.add_node(sim_ec1_ec2[each])\n",
    "    G.add_edge(each, sim_ec1_ec2[each])\n",
    "for each in sim_ec2_ec1:\n",
    "    G.add_node(each)\n",
    "    G.add_node(sim_ec2_ec1[each])\n",
    "    G.add_edge(each, sim_ec2_ec1[each])\n",
    "\n",
    "G_nodes = list(G.nodes)\n",
    "aaabd = list(G.edges)\n",
    "\n",
    "transitive_closures = {}\n",
    "count = 0\n",
    "for each in G_nodes:\n",
    "    cluster = nx.node_connected_component(G, each)\n",
    "    for every in cluster:\n",
    "        G_nodes.remove(every)\n",
    "    transitive_closures[count] = list(cluster)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3463b44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'work': 'employer',\n",
       " 'reportTo': 'manager',\n",
       " 'country': 'country',\n",
       " 'location': 'domicile',\n",
       " 'gender': 'gender',\n",
       " 'headQuarter': 'domicile',\n",
       " 'city': 'location',\n",
       " 'author': 'author',\n",
       " 'corporation': 'company',\n",
       " 'name': 'name',\n",
       " 'company': 'company',\n",
       " 'domicile': 'country',\n",
       " 'field': 'position',\n",
       " 'artist': 'artist',\n",
       " 'corpName': 'corporation',\n",
       " 'CEO': 'manager'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_ec1_ec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ae7785df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'empName': 'name',\n",
       " 'gender': 'gender',\n",
       " 'country': 'country',\n",
       " 'location': 'city',\n",
       " 'author': 'author',\n",
       " 'manager': 'CEO',\n",
       " 'corporation': 'corpName',\n",
       " 'name': 'name',\n",
       " 'company': 'company',\n",
       " 'domicile': 'country',\n",
       " 'artist': 'artist',\n",
       " 'position': 'field',\n",
       " 'employer': 'work'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_ec2_ec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "253f549e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['work', 'employer'],\n",
       " 1: ['manager', 'CEO', 'reportTo'],\n",
       " 2: ['city', 'country', 'location', 'headQuarter', 'domicile'],\n",
       " 3: ['corporation', 'corpName', 'company'],\n",
       " 4: ['position', 'field']}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clusters that is produced\n",
    "transitive_closures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b726378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_by_attribute(ec, attcol):\n",
    "    attribute_cluster_token_temp = {}\n",
    "    for every in attcol:\n",
    "        liste = []\n",
    "        for each in ec:\n",
    "            if every in each:\n",
    "                wtokens = nltk.word_tokenize(str(each[every]))\n",
    "                for word in wtokens:\n",
    "                    liste.append(word)\n",
    "        liste = text_cleaning(liste)\n",
    "        attribute_cluster_token_temp[every] = liste\n",
    "    return attribute_cluster_token_temp\n",
    "\n",
    "\n",
    "attribute_cluster_token_temp_ec1 = tokenize_by_attribute(ec1, attE1Keys)\n",
    "attribute_cluster_token_temp_ec2 = tokenize_by_attribute(ec2, attE2Keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dcdff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_token_test = {}\n",
    "for each in transitive_closures:\n",
    "    liste = []\n",
    "    for every in transitive_closures[each]:\n",
    "        if every in attribute_cluster_token_temp_ec1:\n",
    "            for word in attribute_cluster_token_temp_ec1[every]:\n",
    "                liste.append(word)\n",
    "        if every in attribute_cluster_token_temp_ec2:\n",
    "            for word in attribute_cluster_token_temp_ec2[every]:\n",
    "                liste.append(word)\n",
    "    common_token_test[each] = set(liste)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da43b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_token_dict_by_attribute(ec1, transitive_closures):\n",
    "    dict_att = []\n",
    "    for each in ec1:\n",
    "        att_dict = {}\n",
    "        for every in each:\n",
    "            if every == \"id\":\n",
    "                att_dict[every] = each[every]\n",
    "            for attclu in transitive_closures:\n",
    "                if every in transitive_closures[attclu]:\n",
    "                    tmp = text_cleaning(nltk.word_tokenize(each[every]))\n",
    "                    att_dict[every] = tmp\n",
    "        dict_att.append(att_dict)\n",
    "    return dict_att\n",
    "\n",
    "\n",
    "ec1_token_dict_by_attribute = create_token_dict_by_attribute(ec1, transitive_closures)\n",
    "ec2_token_dict_by_attribute = create_token_dict_by_attribute(ec2, transitive_closures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae4be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Token Blocking by Attribute Clusters\n",
    "\n",
    "\n",
    "e1_dict_token = {}\n",
    "for cluster in common_token_test:\n",
    "    #call transitive closure, get a list of all entity in cluster\n",
    "    kept_attribute = transitive_closures[cluster]\n",
    "    kept_attribute.append('id')\n",
    "    for token in common_token_test[cluster]:\n",
    "        liste = []\n",
    "        for entity in ec1_token_dict_by_attribute:\n",
    "            entity = {k: entity[k] for k in kept_attribute if k in entity.keys()}\n",
    "            #for each entity, delete key that is not in the clusters\n",
    "            #and make a new dictionary entity = newDict\n",
    "            \n",
    "            for each in entity:\n",
    "                if each != 'id' and token in entity[each]:\n",
    "                    liste.append(entity['id'])\n",
    "        e1_dict_token[str(cluster)+token] = list(liste)\n",
    "\n",
    "\n",
    "e2_dict_token = {}\n",
    "for cluster in common_token_test:\n",
    "    kept_attribute = transitive_closures[cluster]\n",
    "    kept_attribute.append('id')\n",
    "    for token in common_token_test[cluster]:\n",
    "        liste = []\n",
    "        for entity in ec2_token_dict_by_attribute:\n",
    "            entity = {k: entity[k] for k in kept_attribute if k in entity.keys()}\n",
    "            for each in entity:\n",
    "                if each != 'id' and token in entity[each]:\n",
    "                    liste.append(entity['id'])\n",
    "        e2_dict_token[str(cluster)+token] = list(liste)\n",
    "\n",
    "all_blocks = []\n",
    "for cluster in common_token_test:\n",
    "    for token in common_token_test[cluster]:\n",
    "        if len(e1_dict_token[str(cluster)+token]) > 0 and len(e2_dict_token[str(cluster)+token]) > 0:\n",
    "            new_block = Block(str(cluster)+\"-\"+token, e1_dict_token[str(cluster)+token], e2_dict_token[str(cluster)+token])\n",
    "            all_blocks.append(new_block)\n",
    "all_blocks.sort(key=lambda x: x.b_cardinality, reverse=True)\n",
    "token_blocks_by_attribute_df = pd.DataFrame([x.as_dict() for x in all_blocks])\n",
    "display(token_blocks_by_attribute_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f50277b",
   "metadata": {},
   "source": [
    "<h3>Meta-blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7fd9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#G1 = Weighting Scheme: Common block, Prunning: WEP\n",
    "#G2 = Weighting Scheme: Jaccard simmilarity, Prunning: WEP\n",
    "#G3 = Weighting Scheme: Common block, Prunning: Cardinality Node Prunning \n",
    "#G4 = Weighting Scheme: Jaccard simmilarity, Prunning: Cardinality Node Prunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8748cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(blocks, weighting_scheme = 'cbs'):\n",
    "    G = nx.Graph()\n",
    "    if weighting_scheme == 'jaccard': #cb\n",
    "        edges = weight_edge_jac(blocks)\n",
    "    else:\n",
    "        edges = weight_edge_cb(blocks)\n",
    "\n",
    "    for nodes in edges:\n",
    "        G.add_edge(nodes[0], nodes[1], weight=edges[nodes])\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    \n",
    "    return G, edges\n",
    "\n",
    "def weight_edge_cb(token_blocks_all):\n",
    "    all_pairs = []\n",
    "    for b in token_blocks_all:\n",
    "        ib1 = b.ib1\n",
    "        ib2 = b.ib2\n",
    "        b_pairs = list(itertools.product(ib1, ib2)) #all possible combinations of the two list\n",
    "        b_pairs = [list(p) for p in b_pairs]\n",
    "        for pair in b_pairs:\n",
    "            pair.sort()\n",
    "        all_pairs.append(b_pairs)\n",
    "\n",
    "    all_pairs_flat = [item for sublist in all_pairs for item in sublist] #flatten it\n",
    "    pairs_dict = Counter(map(tuple, all_pairs_flat))\n",
    "    pairs_dict = OrderedDict(sorted(pairs_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "    return pairs_dict\n",
    "\n",
    "def weight_edge_jac(token_blocks_all):\n",
    "    \n",
    "    all_inner_block = []\n",
    "    for b in token_blocks_all:\n",
    "        ib1 = b.ib1\n",
    "        ib2 = b.ib2\n",
    "        comb_inner_b = list(ib1 + ib2)\n",
    "        all_inner_block.append(comb_inner_b)\n",
    "        \n",
    "    all_block_flat = [item for sublist in all_inner_block for item in sublist]\n",
    "    count_ep = Counter(all_block_flat)\n",
    "    \n",
    "    all_pairs = []\n",
    "    for b in token_blocks_all:\n",
    "        ib1 = b.ib1\n",
    "        ib2 = b.ib2\n",
    "        b_pairs = list(itertools.product(ib1, ib2)) #all possible combinations of the two list\n",
    "        b_pairs = [list(p) for p in b_pairs]\n",
    "        for pair in b_pairs:\n",
    "            pair.sort()\n",
    "        all_pairs.append(b_pairs)\n",
    "\n",
    "    all_pairs_flat = [item for sublist in all_pairs for item in sublist]\n",
    "    pairs_dict = Counter(map(tuple, all_pairs_flat))\n",
    "    pairs_dict = OrderedDict(sorted(pairs_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    for p in pairs_dict:\n",
    "        jac_weight = pairs_dict[p]/(count_ep[p[0]] + count_ep[p[1]] - pairs_dict[p])\n",
    "        pairs_dict[p] = jac_weight\n",
    "    \n",
    "    return pairs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d6c4e3",
   "metadata": {},
   "source": [
    "<h4>Graph 1 (WS: CBS) before prunning with Weight Edge Prunning</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df6b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "G1, g1_edges = create_graph(token_blocks_all)\n",
    "\n",
    "pos = nx.spring_layout(G1, seed=7)  # positions for all nodes - seed for reproducibility\n",
    "\n",
    "#find average weight:\n",
    "tot_weight = []\n",
    "for w in g1_edges:\n",
    "    tot_weight.append(g1_edges[w])\n",
    "average_edge_weight = np.average(tot_weight)\n",
    "print(\"average edge weight\", average_edge_weight)\n",
    "\n",
    "print(\"Before prunning num. of edges:\", len(G1.edges()))\n",
    "print(\"Before prunning num. of nodes:\", len(G1.nodes()))\n",
    "\n",
    "# nodes\n",
    "nx.draw_networkx_nodes(G1, pos, node_size=400)\n",
    "\n",
    "elarge = [(u, v) for (u, v, d) in G1.edges(data=True) if d[\"weight\"] > average_edge_weight]\n",
    "esmall = [(u, v) for (u, v, d) in G1.edges(data=True) if d[\"weight\"] <= average_edge_weight]\n",
    "\n",
    "# edges\n",
    "nx.draw_networkx_edges(G1, pos, edgelist=elarge, width=3, edge_color=\"r\")\n",
    "nx.draw_networkx_edges(G1, pos, edgelist=esmall, width=3, alpha=0.5, edge_color=\"b\", style=\"dashed\")\n",
    "\n",
    "nx.draw_networkx_labels(G1, pos, font_size=20, font_family=\"sans-serif\")\n",
    "\n",
    "ax = plt.gca()\n",
    "plt.title(\"G1 before prunning\")\n",
    "ax.margins(0.08)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eba9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove edges that are below the treshold value (average weight)\n",
    "removed_edges = [(a,b) for a, b, attrs in G1.edges(data=True) if attrs[\"weight\"] <= average_edge_weight]\n",
    "G1.remove_edges_from(removed_edges)\n",
    "\n",
    "#remove isolated nodes\n",
    "G1.remove_nodes_from(list(nx.isolates(G1)))\n",
    "\n",
    "\n",
    "print(\"After prunning num. of edges:\", len(G1.edges()))\n",
    "print(\"After prunning num. of nodes:\", len(G1.nodes()))\n",
    "\n",
    "# nodes\n",
    "nx.draw_networkx_nodes(G1, pos, node_size=400)\n",
    "\n",
    "elarge = [(u, v) for (u, v, d) in G1.edges(data=True) if d[\"weight\"] > average_edge_weight]\n",
    "esmall = [(u, v) for (u, v, d) in G1.edges(data=True) if d[\"weight\"] <= average_edge_weight]\n",
    "\n",
    "# edges\n",
    "nx.draw_networkx_edges(G1, pos, edgelist=elarge, width=3, edge_color=\"r\")\n",
    "nx.draw_networkx_edges(G1, pos, edgelist=esmall, width=3, alpha=0.5, edge_color=\"b\", style=\"dashed\")\n",
    "\n",
    "nx.draw_networkx_labels(G1, pos, font_size=20, font_family=\"sans-serif\")\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.margins(0.08)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"G1 after prunning\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2018e3d",
   "metadata": {},
   "source": [
    "<h4>Graph 2 (WS = Jaccard Sim) before prunning with Weight Edge Prunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3085dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "G2, g2_edges = create_graph(token_blocks_all, \"jaccard\")\n",
    "\n",
    "#make an instance G3 for the second prunning\n",
    "G4 = G2\n",
    "g4_edges = g2_edges\n",
    "\n",
    "pos = nx.spring_layout(G2, seed=7)  # positions for all nodes - seed for reproducibility\n",
    "\n",
    "print(\"Before prunning num. of edges:\", len(G2.edges()))\n",
    "print(\"Before prunning num. of nodes:\", len(G2.nodes()))\n",
    "\n",
    "#find average weight:\n",
    "tot_weight = []\n",
    "for w in g2_edges:\n",
    "    tot_weight.append(g2_edges[w])\n",
    "average_edge_weight = np.average(tot_weight)\n",
    "print(\"average edge weight\", average_edge_weight)\n",
    "\n",
    "# nodes\n",
    "nx.draw_networkx_nodes(G2, pos, node_size=400)\n",
    "\n",
    "elarge = [(u, v) for (u, v, d) in G2.edges(data=True) if d[\"weight\"] > average_edge_weight]\n",
    "esmall = [(u, v) for (u, v, d) in G2.edges(data=True) if d[\"weight\"] <= average_edge_weight]\n",
    "\n",
    "# edges\n",
    "# edges\n",
    "nx.draw_networkx_edges(G2, pos, edgelist=elarge, width=3, edge_color=\"r\")\n",
    "nx.draw_networkx_edges(G2, pos, edgelist=esmall, width=3, alpha=0.5, edge_color=\"b\", style=\"dashed\")\n",
    "nx.draw_networkx_labels(G2, pos, font_size=20, font_family=\"sans-serif\")\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.margins(0.08)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"G2 before prunning\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d092649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove edges that are below the treshold value (average weight)\n",
    "removed_edges = [(a,b) for a, b, attrs in G2.edges(data=True) if attrs[\"weight\"] <= average_edge_weight]\n",
    "G2.remove_edges_from(removed_edges)\n",
    "\n",
    "#remove isolated nodes\n",
    "G2.remove_nodes_from(list(nx.isolates(G2)))\n",
    "\n",
    "\n",
    "print(\"After prunning num. of edges:\", len(G2.edges()))\n",
    "print(\"After prunning num. of nodes:\", len(G2.nodes()))\n",
    "\n",
    "\n",
    "# nodes\n",
    "nx.draw_networkx_nodes(G2, pos, node_size=400)\n",
    "elarge = [(u, v) for (u, v, d) in G2.edges(data=True) if d[\"weight\"] > average_edge_weight]\n",
    "esmall = [(u, v) for (u, v, d) in G2.edges(data=True) if d[\"weight\"] <=average_edge_weight]\n",
    "\n",
    "# edges\n",
    "nx.draw_networkx_edges(G2, pos, edgelist=elarge, width=3, edge_color=\"r\")\n",
    "nx.draw_networkx_edges(G2, pos, edgelist=esmall, width=3, alpha=0.5, edge_color=\"b\", style=\"dashed\")\n",
    "nx.draw_networkx_labels(G2, pos, font_size=20, font_family=\"sans-serif\")\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.margins(0.08)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"G2 after prunning\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409bfb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prunning with node cardinality\n",
    "def cardinality_node_prunning(G):\n",
    "    e_out = []\n",
    "    vertices = G.nodes()\n",
    "    for v in vertices:\n",
    "        sorted_stack = []\n",
    "        edge_weight = {}\n",
    "        \n",
    "        #get neighbors\n",
    "        neighbors = [n for n in G.neighbors(v)]\n",
    "        k = math.ceil(len(neighbors)*0.1) #cardinality treshold\n",
    "\n",
    "    \n",
    "        edges_of_v = list(G.edges(v))\n",
    "        \n",
    "        ev_dict = {}\n",
    "        for ev in edges_of_v:\n",
    "            weight = G[ev[0]][ev[1]]['weight']\n",
    "            ev_dict[(ev[0], ev[1])] = weight\n",
    "        \n",
    "        sorted_ev_dict = {k: v for k, v in sorted(ev_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "        \n",
    "        for edge, weight in sorted_ev_dict.items():\n",
    "            while k > len(sorted_stack):\n",
    "                sorted_stack.append(edge)\n",
    "        \n",
    "        sorted_stack =  list(set(sorted_stack))\n",
    "        \n",
    "        for edge, weight in sorted_ev_dict.items():\n",
    "            if edge in sorted_stack:\n",
    "                e_out.append(edge)\n",
    "    e_out = list(set(e_out))\n",
    "                \n",
    "    return e_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13985aeb",
   "metadata": {},
   "source": [
    "<h4>Graph 3 (WS: CBS) before prunning with Cardinality Node Prunning</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2264e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "G3, g3_edges = create_graph(token_blocks_all)\n",
    "G3.remove_nodes_from(list(nx.isolates(G3)))\n",
    "\n",
    "pos = nx.spring_layout(G3, seed=7)  # positions for all nodes - seed for reproducibility\n",
    "\n",
    "print(\"Before prunning num. of edges:\", len(G3.edges()))\n",
    "print(\"Before prunning num. of nodes:\", len(G3.nodes()))\n",
    "\n",
    "\n",
    "nx.draw_networkx_nodes(G3, pos, node_size=400)\n",
    "# edges\n",
    "nx.draw_networkx_edges(G3, pos, width=3, edge_color=\"r\")\n",
    "nx.draw_networkx_labels(G3, pos, font_size=20, font_family=\"sans-serif\")\n",
    "\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.margins(0.08)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"G3 before prunning\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490582f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retained_edges_G3 = cardinality_node_prunning(G3)\n",
    "original_edges_g3 = list(G3.edges())\n",
    "deleted_edges_g3 = list(set(original_edges_g3) - set(retained_edges_G3))\n",
    "for dedge in deleted_edges_g3:\n",
    "    G3.remove_edge(dedge[0], dedge[1])\n",
    "    \n",
    "#remove isolated nodes\n",
    "G3.remove_nodes_from(list(nx.isolates(G3)))\n",
    "\n",
    "print(\"After prunning num. of edges:\", len(G3.edges()))\n",
    "print(\"After prunning num. of nodes:\", len(G3.nodes()))\n",
    "    \n",
    "nx.draw_networkx_nodes(G3, pos, node_size=400)\n",
    "# edges\n",
    "nx.draw_networkx_edges(G3, pos, width=3, edge_color=\"r\")\n",
    "nx.draw_networkx_labels(G3, pos, font_size=20, font_family=\"sans-serif\")\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.margins(0.08)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"G3 after prunning\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f1a286",
   "metadata": {},
   "source": [
    "<h4>Graph 4 (WS: Jaccard similarity) before prunning with Cardinality Node Prunning</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715cc226",
   "metadata": {},
   "outputs": [],
   "source": [
    "G4, g4_edges = create_graph(token_blocks_all, \"jaccard\")\n",
    "G4.remove_nodes_from(list(nx.isolates(G4)))\n",
    "\n",
    "print(\"Before prunning num. of edges:\", len(G4.edges()))\n",
    "print(\"Before prunning num. of nodes:\", len(G4.nodes()))\n",
    "\n",
    "nx.draw_networkx_nodes(G4, pos, node_size=400)\n",
    "# edges\n",
    "nx.draw_networkx_edges(G4, pos, width=3, edge_color=\"r\")\n",
    "nx.draw_networkx_labels(G4, pos, font_size=20, font_family=\"sans-serif\")\n",
    "\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.margins(0.08)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d47462",
   "metadata": {},
   "outputs": [],
   "source": [
    "retained_edges_g4 = cardinality_node_prunning(G4)\n",
    "original_edges_g4 = list(G4.edges())\n",
    "deleted_edges_g4 = list(set(original_edges_g4) - set(retained_edges_g4))\n",
    "for dedge in deleted_edges_g4:\n",
    "    G4.remove_edge(dedge[0], dedge[1])\n",
    "    \n",
    "#remove isolated nodes\n",
    "G4.remove_nodes_from(list(nx.isolates(G4)))\n",
    "\n",
    "print(\"After prunning num. of edges:\", len(G4.edges()))\n",
    "print(\"After prunning num. of nodes:\", len(G4.nodes()))\n",
    "    \n",
    "nx.draw_networkx_nodes(G4, pos, node_size=400)\n",
    "# edges\n",
    "nx.draw_networkx_edges(G4, pos, width=3, edge_color=\"r\")\n",
    "nx.draw_networkx_labels(G4, pos, font_size=20, font_family=\"sans-serif\")\n",
    "\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.margins(0.08)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"G4 after prunning\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2619ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
